{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First load the dataset into program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Data :  4177\n",
      "\n",
      "Attribute :\n",
      "sex\n",
      "length\n",
      "diameter\n",
      "height\n",
      "whole-weight\n",
      "shucked-weight\n",
      "viscera-weight\n",
      "shell-weight\n",
      "rings\n",
      "\n",
      "Data :\n",
      "['M', '91', '73', '19', '102.8', '44.9', '20.2', '30', '15']\n",
      "['M', '70', '53', '18', '45.1', '19.9', '9.7', '14', '7']\n",
      "['F', '106', '84', '27', '135.4', '51.3', '28.3', '42', '9']\n",
      "['M', '88', '73', '25', '103.2', '43.1', '22.8', '31', '10']\n",
      "['I', '66', '51', '16', '41', '17.9', '7.9', '11', '7']\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "from csv import reader\n",
    "\n",
    "file = \"abalone_original.csv\"\n",
    "attribute = list()\n",
    "dataset = list()\n",
    "data_count = 0\n",
    "with open(file,'r') as abalone_data:\n",
    "    data_reader = reader(abalone_data)\n",
    "    for row in data_reader:\n",
    "        if not row:\n",
    "            continue\n",
    "        if data_count == 0:\n",
    "            attribute = row\n",
    "            data_count += 1\n",
    "        else:\n",
    "            dataset.append(row)\n",
    "            data_count += 1\n",
    "data_count -= 1\n",
    "\n",
    "print(\"Total Data : \", data_count)\n",
    "\n",
    "print(\"\\nAttribute :\")\n",
    "for i in attribute:\n",
    "    print(i)\n",
    "\n",
    "print(\"\\nData :\")\n",
    "for i in range(5):\n",
    "    print(dataset[i])\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the previous results, it's clear that all features have Gaussian Distribution. Next clean the data so that all data become float type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Data :\n",
      "['M', 91.0, 73.0, 19.0, 102.8, 44.9, 20.2, 30.0, 15.0]\n",
      "['M', 70.0, 53.0, 18.0, 45.1, 19.9, 9.7, 14.0, 7.0]\n",
      "['F', 106.0, 84.0, 27.0, 135.4, 51.3, 28.3, 42.0, 9.0]\n",
      "['M', 88.0, 73.0, 25.0, 103.2, 43.1, 22.8, 31.0, 10.0]\n",
      "['I', 66.0, 51.0, 16.0, 41.0, 17.9, 7.9, 11.0, 7.0]\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "for row in range(len(dataset)):\n",
    "    for feature in range(1, len(dataset[row])):\n",
    "        dataset[row][feature] = float(dataset[row][feature].strip())\n",
    "        \n",
    "print(\"Converted Data :\")\n",
    "for i in range(5):\n",
    "    print(dataset[i])\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use 5-fold cross validation to evaluate this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "\n",
    "n_fold = 5\n",
    "fold_size = int(len(dataset) / 5)\n",
    "data_split = list()\n",
    "dataset_copy = list(dataset)\n",
    "\n",
    "for _ in range(n_fold):\n",
    "    fold = list()\n",
    "    while len(fold) < fold_size:\n",
    "        index = randrange(len(dataset_copy))\n",
    "        fold.append(dataset_copy.pop(index))\n",
    "    data_split.append(fold)\n",
    "\n",
    "train_set = list()\n",
    "for fold in data_split:\n",
    "    set = list()\n",
    "    train = list()\n",
    "    val = list()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the set of data we got from folding, we will make a collection of training set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data 1\n",
      "Training Set\n",
      "['M', 120.0, 93.0, 31.0, 252.4, 124.9, 49.1, 66.0, 10.0]\n",
      "['F', 112.0, 92.0, 36.0, 194.0, 68.4, 39.2, 71.0, 12.0]\n",
      "['F', 110.0, 76.0, 33.0, 241.0, 108.6, 58.8, 66.9, 10.0]\n",
      "['M', 106.0, 83.0, 26.0, 168.5, 55.0, 38.9, 53.0, 20.0]\n",
      "['F', 124.0, 95.0, 32.0, 264.9, 137.3, 46.6, 65.5, 9.0]\n",
      "...\n",
      "Test Set\n",
      "[100.0, 77.0, 24.0, 103.2, 39.4, 26.1, 33.0, 8.0]\n",
      "[140.0, 110.0, 39.0, 324.9, 135.0, 69.4, 107.0, 13.0]\n",
      "[97.0, 79.0, 32.0, 132.0, 49.5, 25.6, 47.0, 14.0]\n",
      "[105.0, 75.0, 24.0, 126.3, 60.9, 22.8, 38.0, 9.0]\n",
      "[149.0, 117.0, 38.0, 393.2, 168.7, 87.4, 117.1, 18.0]\n",
      "...\n",
      "\n",
      "Data 2\n",
      "Training Set\n",
      "['I', 100.0, 77.0, 24.0, 103.2, 39.4, 26.1, 33.0, 8.0]\n",
      "['M', 140.0, 110.0, 39.0, 324.9, 135.0, 69.4, 107.0, 13.0]\n",
      "['F', 97.0, 79.0, 32.0, 132.0, 49.5, 25.6, 47.0, 14.0]\n",
      "['I', 105.0, 75.0, 24.0, 126.3, 60.9, 22.8, 38.0, 9.0]\n",
      "['F', 149.0, 117.0, 38.0, 393.2, 168.7, 87.4, 117.1, 18.0]\n",
      "...\n",
      "Test Set\n",
      "[120.0, 93.0, 31.0, 252.4, 124.9, 49.1, 66.0, 10.0]\n",
      "[112.0, 92.0, 36.0, 194.0, 68.4, 39.2, 71.0, 12.0]\n",
      "[110.0, 76.0, 33.0, 241.0, 108.6, 58.8, 66.9, 10.0]\n",
      "[106.0, 83.0, 26.0, 168.5, 55.0, 38.9, 53.0, 20.0]\n",
      "[124.0, 95.0, 32.0, 264.9, 137.3, 46.6, 65.5, 9.0]\n",
      "...\n",
      "\n",
      "Data 3\n",
      "Training Set\n",
      "['I', 100.0, 77.0, 24.0, 103.2, 39.4, 26.1, 33.0, 8.0]\n",
      "['M', 140.0, 110.0, 39.0, 324.9, 135.0, 69.4, 107.0, 13.0]\n",
      "['F', 97.0, 79.0, 32.0, 132.0, 49.5, 25.6, 47.0, 14.0]\n",
      "['I', 105.0, 75.0, 24.0, 126.3, 60.9, 22.8, 38.0, 9.0]\n",
      "['F', 149.0, 117.0, 38.0, 393.2, 168.7, 87.4, 117.1, 18.0]\n",
      "...\n",
      "Test Set\n",
      "[82.0, 66.0, 21.0, 67.0, 30.5, 14.8, 22.0, 7.0]\n",
      "[100.0, 76.0, 22.0, 98.8, 43.6, 18.0, 26.5, 7.0]\n",
      "[59.0, 43.0, 15.0, 25.8, 10.0, 5.9, 8.0, 7.0]\n",
      "[126.0, 103.0, 33.0, 270.4, 97.6, 69.8, 90.0, 20.0]\n",
      "[92.0, 70.0, 22.0, 93.5, 42.5, 19.8, 27.5, 7.0]\n",
      "...\n",
      "\n",
      "Data 4\n",
      "Training Set\n",
      "['I', 100.0, 77.0, 24.0, 103.2, 39.4, 26.1, 33.0, 8.0]\n",
      "['M', 140.0, 110.0, 39.0, 324.9, 135.0, 69.4, 107.0, 13.0]\n",
      "['F', 97.0, 79.0, 32.0, 132.0, 49.5, 25.6, 47.0, 14.0]\n",
      "['I', 105.0, 75.0, 24.0, 126.3, 60.9, 22.8, 38.0, 9.0]\n",
      "['F', 149.0, 117.0, 38.0, 393.2, 168.7, 87.4, 117.1, 18.0]\n",
      "...\n",
      "Test Set\n",
      "[108.0, 84.0, 29.0, 173.1, 86.3, 32.6, 43.5, 10.0]\n",
      "[98.0, 78.0, 28.0, 141.4, 55.9, 43.7, 36.0, 13.0]\n",
      "[117.0, 93.0, 29.0, 197.1, 86.5, 42.9, 56.9, 10.0]\n",
      "[92.0, 74.0, 22.0, 79.3, 29.7, 17.1, 29.1, 8.0]\n",
      "[106.0, 79.0, 29.0, 155.0, 61.6, 33.8, 51.0, 7.0]\n",
      "...\n",
      "\n",
      "Data 5\n",
      "Training Set\n",
      "['I', 100.0, 77.0, 24.0, 103.2, 39.4, 26.1, 33.0, 8.0]\n",
      "['M', 140.0, 110.0, 39.0, 324.9, 135.0, 69.4, 107.0, 13.0]\n",
      "['F', 97.0, 79.0, 32.0, 132.0, 49.5, 25.6, 47.0, 14.0]\n",
      "['I', 105.0, 75.0, 24.0, 126.3, 60.9, 22.8, 38.0, 9.0]\n",
      "['F', 149.0, 117.0, 38.0, 393.2, 168.7, 87.4, 117.1, 18.0]\n",
      "...\n",
      "Test Set\n",
      "[131.0, 109.0, 38.0, 284.9, 126.5, 66.6, 75.6, 10.0]\n",
      "[117.0, 91.0, 33.0, 199.6, 69.0, 49.9, 63.0, 12.0]\n",
      "[124.0, 92.0, 34.0, 225.4, 107.0, 52.7, 59.2, 7.0]\n",
      "[81.0, 50.0, 18.0, 57.5, 25.6, 12.6, 16.1, 7.0]\n",
      "[86.0, 70.0, 18.0, 79.4, 31.5, 17.8, 24.0, 9.0]\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_collection = list()\n",
    "test_collection = list()\n",
    "test_class_collection = list()\n",
    "\n",
    "for i, fold in enumerate(data_split):\n",
    "    train_set = list(data_split)\n",
    "    train_set.remove(fold)\n",
    "    train_set = sum(train_set, [])\n",
    "    test_set = list()\n",
    "    test_set_class = list()\n",
    "    for row in fold:\n",
    "        row_copy = list(row)\n",
    "        test_set_class.append(row_copy[0])\n",
    "        test_set.append(row_copy[1:])\n",
    "    \n",
    "    train_collection.append(train_set)\n",
    "    test_collection.append(test_set)\n",
    "    test_class_collection.append(test_set_class)\n",
    "\n",
    "for i in range(n_fold) :\n",
    "    print(f\"Data {i+1}\")\n",
    "    print(\"Training Set\")\n",
    "    for j in range(5):\n",
    "        print(train_collection[i][j])\n",
    "    print(\"...\")\n",
    "\n",
    "    print(\"Test Set\")\n",
    "    for j in range(5):\n",
    "        print(test_collection[i][j])\n",
    "    print(\"...\\n\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then divide the training set by its class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data 1\n",
      "Training Set\n",
      "[M]\n",
      "[120.0, 93.0, 31.0, 252.4, 124.9, 49.1, 66.0, 10.0]\n",
      "[106.0, 83.0, 26.0, 168.5, 55.0, 38.9, 53.0, 20.0]\n",
      "[122.0, 73.0, 31.0, 215.3, 97.6, 49.8, 54.0, 9.0]\n",
      "[132.0, 99.0, 39.0, 325.5, 118.8, 71.9, 97.0, 10.0]\n",
      "[98.0, 77.0, 25.0, 121.8, 61.3, 19.2, 35.5, 8.0]\n",
      "[F]\n",
      "[112.0, 92.0, 36.0, 194.0, 68.4, 39.2, 71.0, 12.0]\n",
      "[110.0, 76.0, 33.0, 241.0, 108.6, 58.8, 66.9, 10.0]\n",
      "[124.0, 95.0, 32.0, 264.9, 137.3, 46.6, 65.5, 9.0]\n",
      "[135.0, 106.0, 39.0, 299.7, 124.0, 75.0, 85.0, 9.0]\n",
      "[126.0, 101.0, 33.0, 213.0, 91.9, 43.2, 63.0, 12.0]\n",
      "[I]\n",
      "[57.0, 43.0, 12.0, 18.7, 6.2, 4.6, 6.0, 6.0]\n",
      "[87.0, 67.0, 22.0, 76.6, 31.1, 13.5, 27.0, 12.0]\n",
      "[61.0, 43.0, 13.0, 21.5, 8.8, 4.1, 7.6, 5.0]\n",
      "[54.0, 38.0, 16.0, 16.2, 5.3, 3.9, 6.0, 6.0]\n",
      "[101.0, 77.0, 25.0, 119.2, 49.0, 19.4, 42.0, 9.0]\n",
      "...\n",
      "Data 2\n",
      "Training Set\n",
      "[I]\n",
      "[100.0, 77.0, 24.0, 103.2, 39.4, 26.1, 33.0, 8.0]\n",
      "[105.0, 75.0, 24.0, 126.3, 60.9, 22.8, 38.0, 9.0]\n",
      "[85.0, 68.0, 20.0, 74.2, 30.0, 17.3, 23.0, 8.0]\n",
      "[104.0, 77.0, 23.0, 116.2, 51.1, 31.2, 28.6, 10.0]\n",
      "[112.0, 84.0, 28.0, 167.4, 82.8, 42.8, 40.0, 8.0]\n",
      "[M]\n",
      "[140.0, 110.0, 39.0, 324.9, 135.0, 69.4, 107.0, 13.0]\n",
      "[112.0, 90.0, 32.0, 184.4, 86.4, 35.6, 52.0, 15.0]\n",
      "[103.0, 80.0, 25.0, 191.0, 68.2, 50.7, 52.0, 13.0]\n",
      "[89.0, 70.0, 23.0, 72.3, 31.3, 13.9, 23.4, 8.0]\n",
      "[96.0, 74.0, 20.0, 102.7, 48.6, 20.3, 27.0, 8.0]\n",
      "[F]\n",
      "[97.0, 79.0, 32.0, 132.0, 49.5, 25.6, 47.0, 14.0]\n",
      "[149.0, 117.0, 38.0, 393.2, 168.7, 87.4, 117.1, 18.0]\n",
      "[117.0, 93.0, 33.0, 187.1, 80.7, 45.5, 51.8, 9.0]\n",
      "[112.0, 86.0, 25.0, 160.5, 62.6, 34.3, 52.6, 13.0]\n",
      "[116.0, 86.0, 34.0, 296.0, 130.7, 64.8, 83.1, 10.0]\n",
      "...\n",
      "Data 3\n",
      "Training Set\n",
      "[I]\n",
      "[100.0, 77.0, 24.0, 103.2, 39.4, 26.1, 33.0, 8.0]\n",
      "[105.0, 75.0, 24.0, 126.3, 60.9, 22.8, 38.0, 9.0]\n",
      "[85.0, 68.0, 20.0, 74.2, 30.0, 17.3, 23.0, 8.0]\n",
      "[104.0, 77.0, 23.0, 116.2, 51.1, 31.2, 28.6, 10.0]\n",
      "[112.0, 84.0, 28.0, 167.4, 82.8, 42.8, 40.0, 8.0]\n",
      "[M]\n",
      "[140.0, 110.0, 39.0, 324.9, 135.0, 69.4, 107.0, 13.0]\n",
      "[112.0, 90.0, 32.0, 184.4, 86.4, 35.6, 52.0, 15.0]\n",
      "[103.0, 80.0, 25.0, 191.0, 68.2, 50.7, 52.0, 13.0]\n",
      "[89.0, 70.0, 23.0, 72.3, 31.3, 13.9, 23.4, 8.0]\n",
      "[96.0, 74.0, 20.0, 102.7, 48.6, 20.3, 27.0, 8.0]\n",
      "[F]\n",
      "[97.0, 79.0, 32.0, 132.0, 49.5, 25.6, 47.0, 14.0]\n",
      "[149.0, 117.0, 38.0, 393.2, 168.7, 87.4, 117.1, 18.0]\n",
      "[117.0, 93.0, 33.0, 187.1, 80.7, 45.5, 51.8, 9.0]\n",
      "[112.0, 86.0, 25.0, 160.5, 62.6, 34.3, 52.6, 13.0]\n",
      "[116.0, 86.0, 34.0, 296.0, 130.7, 64.8, 83.1, 10.0]\n",
      "...\n",
      "Data 4\n",
      "Training Set\n",
      "[I]\n",
      "[100.0, 77.0, 24.0, 103.2, 39.4, 26.1, 33.0, 8.0]\n",
      "[105.0, 75.0, 24.0, 126.3, 60.9, 22.8, 38.0, 9.0]\n",
      "[85.0, 68.0, 20.0, 74.2, 30.0, 17.3, 23.0, 8.0]\n",
      "[104.0, 77.0, 23.0, 116.2, 51.1, 31.2, 28.6, 10.0]\n",
      "[112.0, 84.0, 28.0, 167.4, 82.8, 42.8, 40.0, 8.0]\n",
      "[M]\n",
      "[140.0, 110.0, 39.0, 324.9, 135.0, 69.4, 107.0, 13.0]\n",
      "[112.0, 90.0, 32.0, 184.4, 86.4, 35.6, 52.0, 15.0]\n",
      "[103.0, 80.0, 25.0, 191.0, 68.2, 50.7, 52.0, 13.0]\n",
      "[89.0, 70.0, 23.0, 72.3, 31.3, 13.9, 23.4, 8.0]\n",
      "[96.0, 74.0, 20.0, 102.7, 48.6, 20.3, 27.0, 8.0]\n",
      "[F]\n",
      "[97.0, 79.0, 32.0, 132.0, 49.5, 25.6, 47.0, 14.0]\n",
      "[149.0, 117.0, 38.0, 393.2, 168.7, 87.4, 117.1, 18.0]\n",
      "[117.0, 93.0, 33.0, 187.1, 80.7, 45.5, 51.8, 9.0]\n",
      "[112.0, 86.0, 25.0, 160.5, 62.6, 34.3, 52.6, 13.0]\n",
      "[116.0, 86.0, 34.0, 296.0, 130.7, 64.8, 83.1, 10.0]\n",
      "...\n",
      "Data 5\n",
      "Training Set\n",
      "[I]\n",
      "[100.0, 77.0, 24.0, 103.2, 39.4, 26.1, 33.0, 8.0]\n",
      "[105.0, 75.0, 24.0, 126.3, 60.9, 22.8, 38.0, 9.0]\n",
      "[85.0, 68.0, 20.0, 74.2, 30.0, 17.3, 23.0, 8.0]\n",
      "[104.0, 77.0, 23.0, 116.2, 51.1, 31.2, 28.6, 10.0]\n",
      "[112.0, 84.0, 28.0, 167.4, 82.8, 42.8, 40.0, 8.0]\n",
      "[M]\n",
      "[140.0, 110.0, 39.0, 324.9, 135.0, 69.4, 107.0, 13.0]\n",
      "[112.0, 90.0, 32.0, 184.4, 86.4, 35.6, 52.0, 15.0]\n",
      "[103.0, 80.0, 25.0, 191.0, 68.2, 50.7, 52.0, 13.0]\n",
      "[89.0, 70.0, 23.0, 72.3, 31.3, 13.9, 23.4, 8.0]\n",
      "[96.0, 74.0, 20.0, 102.7, 48.6, 20.3, 27.0, 8.0]\n",
      "[F]\n",
      "[97.0, 79.0, 32.0, 132.0, 49.5, 25.6, 47.0, 14.0]\n",
      "[149.0, 117.0, 38.0, 393.2, 168.7, 87.4, 117.1, 18.0]\n",
      "[117.0, 93.0, 33.0, 187.1, 80.7, 45.5, 51.8, 9.0]\n",
      "[112.0, 86.0, 25.0, 160.5, 62.6, 34.3, 52.6, 13.0]\n",
      "[116.0, 86.0, 34.0, 296.0, 130.7, 64.8, 83.1, 10.0]\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "for i, training_copy in enumerate(train_collection):\n",
    "    separated_data = dict()\n",
    "    for row in training_copy:\n",
    "        label = row[0]\n",
    "        value = row[1:]\n",
    "        if label not in separated_data:\n",
    "            separated_data[label] = list()\n",
    "        separated_data[label].append(value)\n",
    "    train_collection[i] = separated_data\n",
    "\n",
    "for i, data in enumerate(train_collection) :\n",
    "    print(f\"Data {i+1}\")\n",
    "    print(\"Training Set\")\n",
    "    for class_, value in data.items():\n",
    "        print(f\"[{class_}]\")\n",
    "        if len(value) >= 5:\n",
    "            for j in range(5):\n",
    "                print(value[j])\n",
    "        else:\n",
    "            for j in range(len(value)):\n",
    "                print(value[j])\n",
    "    print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the disttibution of each feature in each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['continuous', 'continuous', 'continuous', 'continuous', 'continuous', 'continuous', 'continuous', 'continuous']\n"
     ]
    }
   ],
   "source": [
    "distribution = list()\n",
    "row_copy = train_collection[0]['M'][0]\n",
    "\n",
    "for feature in row_copy:\n",
    "    if type(feature) == int or float:\n",
    "        distribution.append(\"continuous\")\n",
    "    else:\n",
    "        distribution.append(\"discrete\")\n",
    "\n",
    "print(distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see all features have continuous distribution. So we will use Gaussian Probability Density Function to calculate the feature likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is to measure the statistics (mean and standard deviation) for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data 1\n",
      "[M]\n",
      "[[112.22129086336966, 20.542956695879596, 1193], [87.77032690695725, 16.905507957638935, 1193], [30.354568315171836, 7.047479058221862, 1193], [197.22414082145823, 93.00156699600122, 1193], [85.83880972338643, 43.789636437407296, 1193], [42.923973176865, 20.860847579075013, 1193], [56.377367979882656, 26.016168645404356, 1193], [10.720033528918693, 2.9854922135661512, 1193]]\n",
      "[F]\n",
      "[[115.65352112676057, 17.335509140422836, 1065], [90.84507042253522, 14.221906637367267, 1065], [31.55211267605634, 8.394040903763758, 1065], [208.2143661971835, 86.32932741972284, 1065], [88.80882629107975, 40.15501491439453, 1065], [45.91633802816907, 19.57053648297601, 1065], [60.1556807511737, 25.180481762223767, 1065], [11.145539906103286, 3.151029387024047, 1065]]\n",
      "[I]\n",
      "[[85.1561922365989, 21.794401412830403, 1082], [64.92698706099816, 17.609738706090425, 1082], [21.475046210720887, 6.362603671730947, 1082], [84.84103512014782, 56.312563341113055, 1082], [37.56571164510161, 25.303759376370817, 1082], [18.090110905730125, 12.285704040142424, 1082], [25.366266173752322, 16.81546463231097, 1082], [7.859519408502773, 2.467297103200217, 1082]]\n",
      "\n",
      "\n",
      "Data 2\n",
      "[I]\n",
      "[[85.37001897533207, 22.101760953069114, 1054], [65.19829222011386, 17.88470930963696, 1054], [21.555977229601517, 6.453736106154027, 1054], [86.26622390891838, 56.86638475138419, 1054], [38.07248576850089, 25.651256373697958, 1054], [18.411574952561672, 12.387532361652829, 1054], [25.6591081593928, 16.818029136827253, 1054], [7.8719165085389, 2.4660484188350296, 1054]]\n",
      "[M]\n",
      "[[111.88206388206389, 20.70378553330357, 1221], [87.57330057330057, 17.08931910697175, 1221], [30.06797706797707, 6.703397591150886, 1221], [196.73423423423395, 92.82091636900142, 1221], [85.96363636363633, 44.01078766280432, 1221], [42.716543816543776, 20.589608349345333, 1221], [56.03906633906634, 25.993639930422983, 1221], [10.647010647010648, 3.032455801424234, 1221]]\n",
      "[F]\n",
      "[[116.10140845070423, 17.06078936417334, 1065], [91.06854460093896, 14.061024233144844, 1065], [31.673239436619717, 8.397115471207316, 1065], [210.2385915492962, 85.89842283189421, 1065], [89.61455399061026, 39.62479957034227, 1065], [46.18976525821602, 19.544007081987665, 1065], [60.48563380281689, 24.841727761072644, 1065], [11.11737089201878, 3.0971622711647497, 1065]]\n",
      "\n",
      "\n",
      "Data 3\n",
      "[I]\n",
      "[[85.99720930232559, 21.620025539046864, 1075], [65.69767441860465, 17.52642974098241, 1075], [21.74418604651163, 6.405497752330599, 1075], [87.53544186046511, 58.147744461053094, 1075], [38.779999999999944, 25.90410333967192, 1075], [18.65786046511628, 12.694379211861948, 1075], [25.99804651162792, 17.297723930146713, 1075], [7.927441860465116, 2.553038950321508, 1075]]\n",
      "[M]\n",
      "[[112.37235772357724, 20.58623055975916, 1230], [87.96422764227643, 16.88390517262405, 1230], [30.314634146341465, 7.0200941732482605, 1230], [198.8502439024388, 94.8034546639196, 1230], [87.04577235772355, 44.98523266434377, 1230], [43.21390243902437, 21.10711479020084, 1230], [56.36349593495933, 26.170247999667502, 1230], [10.65609756097561, 3.0087381918595586, 1230]]\n",
      "[F]\n",
      "[[115.92753623188406, 17.2605352914329, 1035], [91.07342995169083, 14.250943140381903, 1035], [31.753623188405797, 8.432240669075009, 1035], [210.25130434782645, 86.23728603480666, 1035], [89.56367149758451, 39.7351813436199, 1035], [46.40801932367154, 19.51814023664904, 1035], [60.84937198067632, 25.549000349345516, 1035], [11.097584541062801, 3.0465670125957494, 1035]]\n",
      "\n",
      "\n",
      "Data 4\n",
      "[I]\n",
      "[[85.45682451253482, 21.983418099093395, 1077], [65.16620241411327, 17.783896107175497, 1077], [21.552460538532962, 6.474878964261308, 1077], [86.0687093779015, 57.982119051069965, 1077], [38.1957288765088, 26.08412470577077, 1077], [18.372423398328724, 12.683344278843023, 1077], [25.429897864438235, 17.061820605882843, 1077], [7.888579387186629, 2.5452465949401173, 1077]]\n",
      "[M]\n",
      "[[112.4458804523425, 20.451924699671274, 1238], [88.02342487883683, 16.794211206704144, 1238], [30.29967689822294, 7.036446282749742, 1238], [199.169224555735, 94.63983105027826, 1238], [87.139337641357, 45.0086656010713, 1238], [43.246042003231025, 21.190705387761263, 1238], [56.57269789983842, 26.295041615718425, 1238], [10.768982229402262, 3.065695256068221, 1238]]\n",
      "[F]\n",
      "[[116.01463414634146, 17.267619525288417, 1025], [91.08975609756098, 14.165378625388362, 1025], [31.690731707317074, 8.471745431461438, 1025], [210.513756097561, 85.85854052164441, 1025], [89.75443902439021, 39.47499136991323, 1025], [46.53268292682933, 19.602625459967875, 1025], [60.58975609756099, 24.79743604514608, 1025], [11.16390243902439, 3.0993422295863975, 1025]]\n",
      "\n",
      "\n",
      "Data 5\n",
      "[I]\n",
      "[[85.76388888888889, 21.357949202354813, 1080], [65.50462962962963, 17.30246589624403, 1080], [21.66759259259259, 6.2978044441103815, 1080], [86.65888888888895, 56.935957507861794, 1080], [38.42166666666664, 25.452541619338362, 1080], [18.480000000000015, 12.477828641219162, 1080], [25.731018518518514, 16.927590183907416, 1080], [7.904629629629629, 2.5246104411138077, 1080]]\n",
      "[M]\n",
      "[[112.33878887070377, 20.42408170165502, 1222], [87.84451718494272, 16.73247755997163, 1222], [30.3240589198036, 7.005419258744806, 1222], [198.87643207855967, 95.31185462715726, 1222], [86.62643207855962, 45.14531561417243, 1222], [43.29418985270049, 21.149048083703676, 1222], [56.52471358428804, 26.419704408640342, 1222], [10.729950900163667, 3.0436200639058675, 1222]]\n",
      "[F]\n",
      "[[115.39595375722543, 17.23461853905871, 1038], [90.65703275529866, 14.258121361010703, 1038], [31.34200385356455, 5.969351861877576, 1038], [207.3361271676302, 85.98007088519168, 1038], [88.45510597302493, 39.66104491852503, 1038], [45.651926782273556, 19.376692974130748, 1038], [59.93737957610786, 25.277134511172317, 1038], [11.122350674373795, 3.1260040183612223, 1038]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "from math import exp\n",
    "from math import pi\n",
    "\n",
    "statistics = list()\n",
    "\n",
    "def continuous_convert(dataset):\n",
    "    convert_data = list()\n",
    "    for data in dataset:\n",
    "        convert_data.append(float(data))\n",
    "    return convert_data\n",
    "\n",
    "def mean(feature):\n",
    "    return (sum(feature) / float(len(feature)))\n",
    "\n",
    "def stdv(feature, mean):\n",
    "    variance = sum([(x - mean)**2 for x in feature]) / float(len(feature)-1)\n",
    "    return (sqrt(variance))\n",
    "\n",
    "for dataset in train_collection:\n",
    "    data_statistics = dict()\n",
    "    for class_, data in dataset.items():\n",
    "        data_statistics[class_] = list()\n",
    "        \n",
    "        for feature in zip(*data):\n",
    "            measurement = list()\n",
    "            convert_feature = continuous_convert(feature)\n",
    "            avg = mean(convert_feature)\n",
    "            dev = stdv(convert_feature, avg)\n",
    "            \n",
    "            measurement.append(avg)\n",
    "            measurement.append(dev)\n",
    "            measurement.append(len(feature))\n",
    "            \n",
    "            data_statistics[class_].append(measurement)\n",
    "            \n",
    "    statistics.append(data_statistics)\n",
    "    \n",
    "for i, stat in enumerate(statistics):\n",
    "    print(f\"Data {i + 1}\")\n",
    "    for class_, data in stat.items():\n",
    "        print(f\"[{class_}]\")\n",
    "        print(data)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create Gaussian PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def continuous_probability(value, mean, stdv):\n",
    "    exponent = exp(-((value - mean)**2 / (2 * stdv**2)))\n",
    "    return (1 / (sqrt((2 * pi) * (stdv**2)))) * exponent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is testing our model with test data and get a set of probabilities for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data 1\n",
      "Test Set\n",
      "{'M': 7.836196187602342e-13, 'F': 2.5724621487099925e-13, 'I': 5.206702167991545e-11}\n",
      "{'M': 1.3115829796488392e-14, 'F': 3.1189673252913343e-14, 'I': 4.537441320149523e-32}\n",
      "{'M': 2.0545616518130565e-12, 'F': 1.1458800671134238e-12, 'I': 2.1498245864904232e-13}\n",
      "{'M': 1.9593405571423156e-12, 'F': 8.149924827791228e-13, 'I': 2.1233510447419965e-11}\n",
      "{'M': 2.4829319443479858e-18, 'F': 4.723500073393345e-18, 'I': 3.191727910546518e-45}\n",
      "...\n",
      "\n",
      "Data 2\n",
      "Test Set\n",
      "{'I': 2.907913338516891e-19, 'M': 7.0812341464234565e-12, 'F': 1.1458721293152168e-11}\n",
      "{'I': 1.9940599381499953e-16, 'M': 7.575284120410799e-12, 'F': 1.3145655151414366e-11}\n",
      "{'I': 1.0346943836048116e-18, 'M': 6.169312697648551e-12, 'F': 7.093115193625084e-12}\n",
      "{'I': 3.94063843773173e-18, 'M': 8.008372201729765e-14, 'F': 1.1078721863039235e-13}\n",
      "{'I': 2.857524635682692e-20, 'M': 3.747515239410549e-12, 'F': 5.8554468495311435e-12}\n",
      "...\n",
      "\n",
      "Data 3\n",
      "Test Set\n",
      "{'I': 8.509567710692997e-11, 'M': 1.2824462140254888e-14, 'F': 8.018462007251666e-16}\n",
      "{'I': 6.892070496616506e-11, 'M': 2.1498484776799055e-13, 'F': 4.9524754846190566e-14}\n",
      "{'I': 1.289812679570997e-12, 'M': 1.116043704975582e-18, 'F': 3.1633824501984664e-21}\n",
      "{'I': 1.0774876817569771e-27, 'M': 8.908867196878227e-15, 'F': 3.293962178427693e-14}\n",
      "{'I': 9.570201183626526e-11, 'M': 1.2189671464546812e-13, 'F': 1.82018942031178e-14}\n",
      "...\n",
      "\n",
      "Data 4\n",
      "Test Set\n",
      "{'I': 2.427578372303716e-13, 'M': 1.0206789341994311e-11, 'F': 8.070868328633378e-12}\n",
      "{'I': 3.284601483543283e-13, 'M': 3.4300020624914658e-12, 'F': 1.8142102514150717e-12}\n",
      "{'I': 3.395284625291061e-15, 'M': 1.3383504636169214e-11, 'F': 1.7383134390025884e-11}\n",
      "{'I': 8.430706410685843e-11, 'M': 1.1492244935249684e-13, 'F': 1.6612579236636077e-14}\n",
      "{'I': 1.2967827080987616e-12, 'M': 3.870153345537108e-12, 'F': 2.2180348679044358e-12}\n",
      "...\n",
      "\n",
      "Data 5\n",
      "Test Set\n",
      "{'I': 6.389169413097733e-25, 'M': 4.456110916032048e-13, 'F': 8.184369257537475e-13}\n",
      "{'I': 1.9994613116191471e-16, 'M': 1.0419661079140883e-11, 'F': 2.284008101180003e-11}\n",
      "{'I': 4.955678105239397e-18, 'M': 3.9574548851679435e-12, 'F': 7.93418369493727e-12}\n",
      "{'I': 3.903152052763028e-11, 'M': 5.166225184820765e-16, 'F': 7.576720991846876e-18}\n",
      "{'I': 9.0566937895102e-11, 'M': 3.264431375442665e-14, 'F': 2.364058533033878e-15}\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for collecPtr, test_data in enumerate(test_collection):\n",
    "    total = len(dataset)\n",
    "    \n",
    "    for dataPtr, data in enumerate(test_data):\n",
    "        probabilities = dict()\n",
    "        \n",
    "        for class_, stat in statistics[collecPtr].items():\n",
    "            probabilities[class_] = stat[0][2] / float(total)\n",
    "            \n",
    "            for feature_ptr, feature in enumerate(data):\n",
    "                probabilities[class_] *= continuous_probability(feature\n",
    "                                                               ,stat[feature_ptr][0]\n",
    "                                                               ,stat[feature_ptr][1])\n",
    "        \n",
    "        test_collection[collecPtr][dataPtr] = probabilities\n",
    "\n",
    "for i, data in enumerate(test_collection):\n",
    "    print(f\"Data {i+1}\")\n",
    "    print(\"Test Set\")\n",
    "    for j in range(5):\n",
    "        print(data[j])\n",
    "    print(\"...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After calculating probabilities for all classes, we need to determine which class has teh largest value among others and set that class to be the predicted class for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data 1\n",
      "Test Set\n",
      "I\n",
      "F\n",
      "M\n",
      "I\n",
      "F\n",
      "...\n",
      "\n",
      "Data 2\n",
      "Test Set\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "F\n",
      "...\n",
      "\n",
      "Data 3\n",
      "Test Set\n",
      "I\n",
      "I\n",
      "I\n",
      "F\n",
      "I\n",
      "...\n",
      "\n",
      "Data 4\n",
      "Test Set\n",
      "M\n",
      "M\n",
      "F\n",
      "I\n",
      "M\n",
      "...\n",
      "\n",
      "Data 5\n",
      "Test Set\n",
      "F\n",
      "F\n",
      "F\n",
      "I\n",
      "I\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for collecPtr, test_data in enumerate(test_collection):\n",
    "    for dataPtr, data in enumerate(test_data):\n",
    "        best_class, best_prob = None, -1\n",
    "        for class_, prob in data.items():\n",
    "            if prob > best_prob:\n",
    "                best_prob = prob\n",
    "                best_class = class_\n",
    "        \n",
    "        test_collection[collecPtr][dataPtr] = best_class\n",
    "\n",
    "for i, data in enumerate(test_collection):\n",
    "    print(f\"Data {i+1}\")\n",
    "    print(\"Test Set\")\n",
    "    for j in range(5):\n",
    "        print(data[j])\n",
    "    print(\"...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the last, we will calculate accuracy metric by comparing predicted_class and expected_class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores : [0.511377245508982, 0.5089820359281437, 0.5221556886227545, 0.5317365269461077, 0.5305389221556887]\n",
      "\n",
      "Mean Accuracy : 0.5209580838323353\n"
     ]
    }
   ],
   "source": [
    "scores = list()\n",
    "for i in range(len(test_collection)):\n",
    "    correct = 0\n",
    "    for j in range(len(test_collection[i])):\n",
    "        if test_collection[i][j] == test_class_collection[i][j]:\n",
    "            correct += 1\n",
    "    \n",
    "    accuracy = correct / len(test_collection[i])\n",
    "    scores.append(accuracy)\n",
    "\n",
    "print(f\"Scores : {scores}\\n\")\n",
    "print(f\"Mean Accuracy : {sum(scores) / len(scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result, it can be seen that this model has poor accuracy in detecting sex of Abalone. I hope that in the future I can improve this model even better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
